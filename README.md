# LLM Agent Analysis

This repository contains the codebase and experimental setup for the project titled:

**"A Comparative Experimental Analysis of LLM Capabilities in Single-Agent and Multi-Agent Systems"**

##  Overview

This project evaluates modern LLMs like GPT-4, Claude, Mistral, and LLaMA on benchmarks for both:
- Single-agent scenarios (AgentBench, GAIA)
- Multi-agent scenarios (MultiAgentBench, BattleAgentBench)

## ğŸ“ Repository Structure

```
LLM-Agent-Analysis/
â”œâ”€â”€ scripts/               # Scripts to run and evaluate benchmarks
â”œâ”€â”€ configs/               # Model and environment configurations
â”œâ”€â”€ results/
â”‚   â””â”€â”€ raw/               # Raw results data
â”œâ”€â”€ docs/                  # Project report, figures, comparison matrix
â””â”€â”€ README.md              # This file
```

##  Getting Started

1. Clone the repo:
```bash
git clone https://github.com/riyadadlani02/LLM-Agent-Analysis.git
cd LLM-Agent-Analysis
```

2. Set up the environment:
```bash

conda create -n llm_agents python=3.10
conda activate llm_agents
pip install -r requirements.txt
```

3. Run a benchmark:
```bash
python scripts/run_agentbench.py
```

##  Outputs
- Detailed evaluation logs
- Comparison matrix in `docs/`
- Reproducibility checklist

## ğŸ“„ License
MIT License
